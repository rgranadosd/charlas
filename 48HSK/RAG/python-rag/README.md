# Python RAG System

RAG (Retrieval Augmented Generation) system implemented in Python that combines local embedding models with LLM APIs to provide document-based answers.

## ğŸ“‹ Description

This RAG system allows:
- Loading and processing text documents
- Generating embeddings using local models (Mistral E5 Multilingual)
- Performing semantic search on documents
- Generating answers using LLMs (Groq or OpenAI)

## âœ¨ Features

- **Local Embeddings:** Uses the `intfloat/multilingual-e5-large` model that runs locally (free)
- **Multilingual Support:** The E5 model supports multiple languages
- **Groq Integration:** Uses Groq for fast and free inference
- **OpenAI Integration:** Optional support for OpenAI GPT
- **Included Demos:** Automatic and interactive demos
- **Document Processing:** Loads and processes documents from directories

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- Access to Groq API (free) or OpenAI API

### Configuration

1. **Create virtual environment:**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure credentials:**
   
   Create a `.env` file in the `python-rag/` directory:
   ```bash
   # Required for Groq (free)
   GROQ-TOKEN=your_groq_token_here
   
   # Optional for OpenAI
   OPENAI_API_KEY=your_openai_key_here
   ```
   
   âš ï¸ **IMPORTANT:** The `.env` file is protected by `.gitignore` and will not be uploaded to the repository.

4. **Prepare documents:**
   
   Place your text documents in `data/documents/`. The system will automatically load all `.txt` files in the directory.

## ğŸ¯ Usage

### Automatic Demo

Run an automatic demonstration that shows the complete RAG flow:

```bash
python demo_auto.py
```

This demo:
1. Loads documents from `data/documents/`
2. Generates embeddings
3. Asks an example question
4. Searches for relevant information
5. Generates an answer using Groq

### Interactive Demo

Run an interactive demo where you can ask questions:

```bash
python demo_interactiva.py
```

This demo allows:
- Asking questions about loaded documents
- Viewing relevant chunks found
- Getting answers generated by the LLM

### Using the System Programmatically

```python
from src.rag_system import RAGSystem

# Initialize the system
rag = RAGSystem()

# Ask a question
answer = rag.answer_question("What is the main theme of the document?")
print(answer)
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Documents  â”‚â”€â”€â”€â–¶â”‚  Document    â”‚â”€â”€â”€â–¶â”‚   Text       â”‚
â”‚  (.txt)     â”‚    â”‚   Loader     â”‚    â”‚  Processor   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM       â”‚â—€â”€â”€â”€â”‚   Retrieval  â”‚â—€â”€â”€â”€â”‚ Embeddings  â”‚
â”‚  (Groq/     â”‚    â”‚    System     â”‚    â”‚  Manager    â”‚
â”‚  OpenAI)    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

- **DocumentLoader:** Loads documents from the file system
- **TextProcessor:** Splits documents into chunks for processing
- **EmbeddingsManager:** Generates embeddings using local models or APIs
- **RetrievalSystem:** Searches for relevant chunks using cosine similarity
- **RAGSystem:** Orchestrates all components

## ğŸ“ Project Structure

```
python-rag/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ documents/          # Place your .txt documents here
â”‚       â””â”€â”€ story.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_loader.py  # Loads documents
â”‚   â”œâ”€â”€ text_processor.py   # Processes and splits text
â”‚   â”œâ”€â”€ embeddings_manager.py  # Generates embeddings
â”‚   â”œâ”€â”€ retrieval_system.py    # Semantic search
â”‚   â””â”€â”€ rag_system.py          # Main RAG system
â”œâ”€â”€ demo_auto.py            # Automatic demo
â”œâ”€â”€ demo_interactiva.py     # Interactive demo
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ run-demo.sh            # Execution script
â””â”€â”€ README.md              # This file
```

## ğŸ”§ Advanced Configuration

### Using OpenAI instead of Groq

Modify `demo_auto.py` or `demo_interactiva.py` to use OpenAI:

```python
# Instead of Groq
from openai import OpenAI
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
```

### Changing the Embedding Model

The default model is `intfloat/multilingual-e5-large`. To change:

```python
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('model-name')
```

### Adjusting Chunk Size

In `src/text_processor.py`, you can adjust:
- `chunk_size`: Size of each chunk
- `chunk_overlap`: Overlap between chunks

## ğŸ”’ Security

- âœ… All `.env` files are in `.gitignore`
- âœ… Credentials are never hardcoded in the code
- âœ… Documents are processed locally
- âš ï¸ Embeddings are generated locally (no data sent to external services)

## ğŸ› Troubleshooting

### Error: "Variable 'GROQ-TOKEN' not found"
- Verify that the `.env` file exists in the `python-rag/` directory
- Confirm that the variable is named exactly `GROQ-TOKEN` (with hyphen)
- Restart the script after creating/modifying `.env`

### Error: "File not found: data/documents/story.txt"
- Create the `data/documents/` directory if it doesn't exist
- Place at least one `.txt` file in that directory

### Error loading embedding model
- The model downloads automatically the first time
- Make sure you have internet connection
- The model requires several GB of disk space

### Slow responses
- The embedding model loads into memory (may take a few seconds)
- The first run is slower (model download)
- Consider using Groq for faster responses

## ğŸ“š References

- [Sentence Transformers](https://www.sbert.net/)
- [Groq API](https://groq.com/)
- [OpenAI API](https://platform.openai.com/)
- [Mistral E5 Model](https://huggingface.co/intfloat/multilingual-e5-large)

## ğŸ“ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for more details.
